{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Support\n",
    "\n",
    "This notebook demonstrates PDF processing support for both `Call` and `Chat` objects. PDFs can be provided as URLs or local file paths with various parsing engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from irouter import Call, Chat\n",
    "from irouter.base import nb_markdown\n",
    "\n",
    "# To load OPENROUTER_API_KEY from .env file create a .env file at the root of the project with OPENROUTER_API_KEY=your_api_key\n",
    "# Alternatively pass api_key=your_api_key to the Call or Chat class\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the free tier model for PDF processing demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"moonshotai/kimi-k2:free\"\n",
    "pdf_url = \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Call(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The main contribution of the paper is the introduction of the **Transformer**, a novel neural network architecture for sequence transduction tasks **based entirely on attention mechanisms**, replacing the commonly used recurrent or convolutional layers. This design enables:\n",
       "\n",
       "1. **Superior performance** compared to prior state-of-the-art models (e.g., 28.4 BLEU on WMT 2014 English-to-German and 41.0 BLEU on English-to-French), **even outperforming ensembles**.\n",
       "2. **Greater parallelizability**, allowing **faster training** (e.g., 12 hours on 8 GPUs for the base model).\n",
       "3. **Elimination of sequential recurrence**, addressing the bottleneck of sequential computation in RNNs and enabling more efficient processing.\n",
       "\n",
       "The paper also introduces key innovations like **multi-head self-attention**, **scaled dot-product attention**, and **positional encodings** to integrate sequence order without recurrence."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_markdown(c([pdf_url, \"What is the main contribution of this paper?\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Parsing configuration\n",
    "\n",
    "You can specify different PDF parsing engines using the `extra_body` parameter. For example, use `pdf-text` for free parsing. Check [this docs page](https://openrouter.ai/docs/features/multimodal/pdfs#plugin-configuration) for more details on plugin configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_body = {\"plugins\": [{\"id\": \"file-parser\", \"pdf\": {\"engine\": \"pdf-text\"}}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The paper “Attention Is All You Need” presents **Transformer**, the first fully attention-based sequence-to-sequence model, eliminating recurrence and convolution entirely. Its key innovations are:\n",
       "\n",
       "1. **Multi-Head Self-Attention**  \n",
       "   - Replaces RNN/CNN layers with multiple parallel attention “heads” that jointly attend to information from different representation subspaces, enabling richer modeling and constant-time path lengths between any two positions.\n",
       "\n",
       "2. **Scaled Dot-Product Attention**  \n",
       "   - Introduces a simple, fast attention mechanism (matmul + scale + softmax) that avoids vanishing gradients by scaling dot-products by 1/√dₖ.\n",
       "\n",
       "3. **Pure Attention Architecture**  \n",
       "   - Encoder: 6 identical layers, each with (a) multi-head self-attention and (b) position-wise feed-forward network.  \n",
       "   - Decoder: same 6 layers plus (c) encoder-decoder attention, plus autoregressive masking to prevent future-token leakage.\n",
       "\n",
       "4. **Positional Encoding**  \n",
       "   - Adds fixed sinusoidal position embeddings to word embeddings, letting the model exploit token order without recurrence or convolution and generalize to unseen sequence lengths.\n",
       "\n",
       "5. **Massive Parallelization & Fast Training**  \n",
       "   - Removing recurrence allows full sequence parallelization inside each example. Training a **big** Transformer on 8 P100 GPUs for 3.5 days beats the prior best single-model BLEU scores by **+2.0 (EN→DE)** and achieves **41.0 (EN→FR)**—both with markedly lower FLOPs than previous models.\n",
       "\n",
       "6. **Residual/Layer-Norm Stacking**  \n",
       "   - Each sub-layer uses residual connections and layer normalization, stabilizing deep stacks without recurrence.\n",
       "\n",
       "7. **Shared Embedding Matrix**  \n",
       "   - Ties input/output embedding weights and the pre-softmax projection matrix, reducing parameters.\n",
       "\n",
       "Together, these advances yield the first sequence-to-sequence architecture where **attention is the only mechanism**, surpassing state-of-the-art results while training faster and scaling better."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_markdown(c([pdf_url, \"Summarize the key innovations in this paper.\"], extra_body=extra_body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This paper is about a new deep-learning architecture called the **Transformer**, which revolutionizes sequence-processing tasks (like machine translation) by **replacing recurrent or convolutional layers entirely with an attention mechanism**.\\n\\n### Key Contributions and Concepts:\\n1. **Problem Addressed**:  \\n   Traditional models (RNNs, CNNs) process sequences **sequentially**, limiting parallelization and long-range dependency modeling. Convolutions also struggle with distant relationships in sequences.\\n\\n2. **Solution**:  \\n   The **Transformer** uses **only attention mechanisms (self-attention and multi-head attention)** to capture relationships between all positions in a sequence **in parallel**, making it faster to train and better at handling long-range dependencies.\\n\\n3. **Core Innovations**:\\n   - **Scaled Dot-Product Attention**: A fast, parallelizable attention mechanism.\\n   - **Multi-Head Attention**: Learns multiple types of relationships simultaneously.\\n   - **Positional Encoding**: Injects sequence order info since the model has no recurrence/convolution.\\n\\n4. **Results**:  \\n   On the **WMT 2014 English-German** and **English-French** translation tasks, the Transformer achieved:\\n   - **28.4 BLEU** (En-De, surpassing all prior models, including ensembles).\\n   - **41.0 BLEU** (En-Fr, a new state-of-the-art for single models).  \\n   Crucially, it achieved this **with much lower training time and cost** (e.g., 3.5 days on 8 GPUs vs. weeks for RNN-based models).\\n\\n5. **Broader Impact**:  \\n   The paper laid the foundation for modern large-scale models (e.g., GPT, BERT) by proving that attention alone can outperform complex recurrent or convolutional approaches.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([pdf_url, \"What is this paper about?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask follow-up questions about the PDF, and the chat will maintain context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Key advantages of the Transformer/self-attention over RNNs:\\n\\n1. **Parallelization**:  \\n   RNNs process tokens step-by-step and must wait for the hidden state \\u2006\\\\( h_{t-1} \\\\)\\u2006 before computing \\\\( h_t \\\\), making sequential execution unavoidable. A Transformer layer computes attention over all positions simultaneously; the minimum **number of sequential operations is O(1) instead of O(n)**.\\n\\n2. **Shorter long-range paths**:  \\n   A signal from position \\\\( i \\\\) to position \\\\( j \\\\) in an RNN has to traverse \\\\( O(n) \\\\) steps; self-attention creates a direct edge of constant length, so **maximum path length is O(1)** rather than O(n), making long-distance dependencies easier to learn.\\n\\n3. **Computation when \\\\( n < d \\\\)**:  \\n   For typical translation sequences represented with word-piece/byte-pair tokens, \\\\( n \\\\) (sequence length) is much smaller than \\\\( d \\\\) (hidden dimension), so the self-attention layer’s total complexity \\\\( O(n^2 d) \\\\) can actually be **comparable to or lower** than an RNN layer’s \\\\( O(n d^2) \\\\).\\n\\nBonus (mentioned in the paper):  \\nDrawing attention distributions makes the model more interpretable—individual heads capture syntactic/semantic patterns that are not explicitly visible in recurrent states.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"What are the key advantages of this approach over RNNs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'file',\n",
       "    'file': {'filename': 'document.pdf',\n",
       "     'file_data': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'}},\n",
       "   {'type': 'text', 'text': 'What is this paper about?'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': 'This paper is about a new deep-learning architecture called the **Transformer**, which revolutionizes sequence-processing tasks (like machine translation) by **replacing recurrent or convolutional layers entirely with an attention mechanism**.\\n\\n### Key Contributions and Concepts:\\n1. **Problem Addressed**:  \\n   Traditional models (RNNs, CNNs) process sequences **sequentially**, limiting parallelization and long-range dependency modeling. Convolutions also struggle with distant relationships in sequences.\\n\\n2. **Solution**:  \\n   The **Transformer** uses **only attention mechanisms (self-attention and multi-head attention)** to capture relationships between all positions in a sequence **in parallel**, making it faster to train and better at handling long-range dependencies.\\n\\n3. **Core Innovations**:\\n   - **Scaled Dot-Product Attention**: A fast, parallelizable attention mechanism.\\n   - **Multi-Head Attention**: Learns multiple types of relationships simultaneously.\\n   - **Positional Encoding**: Injects sequence order info since the model has no recurrence/convolution.\\n\\n4. **Results**:  \\n   On the **WMT 2014 English-German** and **English-French** translation tasks, the Transformer achieved:\\n   - **28.4 BLEU** (En-De, surpassing all prior models, including ensembles).\\n   - **41.0 BLEU** (En-Fr, a new state-of-the-art for single models).  \\n   Crucially, it achieved this **with much lower training time and cost** (e.g., 3.5 days on 8 GPUs vs. weeks for RNN-based models).\\n\\n5. **Broader Impact**:  \\n   The paper laid the foundation for modern large-scale models (e.g., GPT, BERT) by proving that attention alone can outperform complex recurrent or convolutional approaches.'},\n",
       " {'role': 'user',\n",
       "  'content': 'What are the key advantages of this approach over RNNs?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Key advantages of the Transformer/self-attention over RNNs:\\n\\n1. **Parallelization**:  \\n   RNNs process tokens step-by-step and must wait for the hidden state \\u2006\\\\( h_{t-1} \\\\)\\u2006 before computing \\\\( h_t \\\\), making sequential execution unavoidable. A Transformer layer computes attention over all positions simultaneously; the minimum **number of sequential operations is O(1) instead of O(n)**.\\n\\n2. **Shorter long-range paths**:  \\n   A signal from position \\\\( i \\\\) to position \\\\( j \\\\) in an RNN has to traverse \\\\( O(n) \\\\) steps; self-attention creates a direct edge of constant length, so **maximum path length is O(1)** rather than O(n), making long-distance dependencies easier to learn.\\n\\n3. **Computation when \\\\( n < d \\\\)**:  \\n   For typical translation sequences represented with word-piece/byte-pair tokens, \\\\( n \\\\) (sequence length) is much smaller than \\\\( d \\\\) (hidden dimension), so the self-attention layer’s total complexity \\\\( O(n^2 d) \\\\) can actually be **comparable to or lower** than an RNN layer’s \\\\( O(n d^2) \\\\).\\n\\nBonus (mentioned in the paper):  \\nDrawing attention distributions makes the model more interpretable—individual heads capture syntactic/semantic patterns that are not explicitly visible in recurrent states.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
