{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Support\n",
    "\n",
    "This notebook demonstrates PDF processing support for both `Call` and `Chat` objects. PDFs can be provided as URLs or local file paths with various parsing engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from irouter import Call, Chat\n",
    "from irouter.base import nb_markdown\n",
    "\n",
    "# To load OPENROUTER_API_KEY from .env file create a .env file at the root of the project with OPENROUTER_API_KEY=your_api_key\n",
    "# Alternatively pass api_key=your_api_key to the Call or Chat class\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we use Moonshot AI's `kimi-k2` model.\n",
    "\n",
    "If the selected LLM has native file processing capabilities, that parser be used. Else the `mistral-ocr` parser is used, which has some small costs associated with it.\n",
    "\n",
    "Under the `PDF Parsing Configuration` section in this notebook you can see how to configure a (free) PDF parsing engine. For more details on PDF support in OpenRouter and pricing, check [this docs page](https://openrouter.ai/docs/features/images-and-pdfs#pdf-support).\n",
    "\n",
    "To see an overview of which LLMs support file input, check the [OpenRouter Model Overview](https://openrouter.ai/models?fmt=cards&input_modalities=audio%2Cfile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"moonshotai/kimi-k2:free\"\n",
    "# The \"Attention Is All You Need\" paper\n",
    "pdf_url = \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will ask the LLM questions about the \"Attention Is All You Need\" paper.\n",
    "\n",
    "<img src=\"https://nlp.seas.harvard.edu/images/the-annotated-transformer_0_0.png\" width=\"300\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF URL\n",
    "\n",
    "The simplest way to work with PDF files in `irouter` is to pass the URL of the PDF file and instruction as a list of strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Call(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The main contribution of this paper is the introduction of the **Transformer**, a novel neural network architecture for sequence transduction that **relies entirely on attention mechanisms, eliminating the need for recurrence or convolution**. This model is designed to be more **parallelizable**, **computationally efficient**, and **effective** than existing recurrent or convolutional approaches. Specifically:\n",
       "\n",
       "1. **Architecture**: The Transformer replaces recurrent and convolutional layers with **multi-head self-attention**, allowing the model to process sequences in parallel and efficiently capture global dependencies.\n",
       "\n",
       "2. **Performance**: The model achieves **state-of-the-art results** on WMT 2014 English-to-German (28.4 BLEU) and English-to-French (41.0 BLEU) translation tasks, **outperforming prior ensembles** while requiring **significantly less training time**.\n",
       "\n",
       "3. **Efficiency**: The Transformer reduces computational complexity and training time through its **parallelizable** design, as it avoids the sequential nature of RNNs and the fixed kernel limitations of CNNs.\n",
       "\n",
       "In summary, the paper demonstrates that **\"attention is all you need\"**—self-attention alone can effectively model sequence relationships while offering major advantages in speed and scalability."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_markdown(c([pdf_url, \"What is the main contribution of this paper?\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Parsing configuration\n",
    "\n",
    "You can specify different PDF parsing engines using the `extra_body` parameter. For example, use the `pdf-text` engine for free parsing. Check [this docs page](https://openrouter.ai/docs/features/multimodal/pdfs#plugin-configuration) for more details on plugin configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_body = {\"plugins\": [{\"id\": \"file-parser\", \"pdf\": {\"engine\": \"pdf-text\"}}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The paper introduces the **Transformer**, a novel neural network architecture that abandons recurrence and convolution entirely, relying solely on **self-attention mechanisms** for sequence transduction tasks like machine translation. Key innovations include:\n",
       "\n",
       "1. **Self-Attention as the Core Mechanism**:  \n",
       "   - Replaces RNNs/CNNs with **multi-head self-attention**, enabling direct modeling of dependencies between any two positions in a sequence, regardless of their distance.  \n",
       "   - Uses **scaled dot-product attention** with a scaling factor \\( \\frac{1}{\\sqrt{d_k}} \\) to mitigate gradient vanishing for high-dimensional keys.\n",
       "\n",
       "2. **Multi-Head Attention**:  \n",
       "   - Parallel attention \"heads\" allow the model to jointly focus on information from different representation subspaces, improving depth and expressivity without significant computational overhead.\n",
       "\n",
       "3. **Positional Encoding**:  \n",
       "   - **Sinusoidal positional encodings** are added to token embeddings to inject sequence order information, eliminating the need for recurrence.  \n",
       "   - Functions are chosen to enable extrapolation to longer sequences.\n",
       "\n",
       "4. **Architecture Details**:  \n",
       "   - **Encoder**: 6 identical layers, each with multi-head self-attention + position-wise feed-forward networks (FFNs).  \n",
       "   - **Decoder**: 6 layers with additional encoder-decoder attention and **causal masking** to preserve auto-regressive properties.\n",
       "\n",
       "5. **Efficiency**:  \n",
       "   - Enables **massive parallelization** (O(1) sequential operations per layer vs. O(n) for RNNs).  \n",
       "   - **Faster training**: Achieves state-of-the-art BLEU scores (28.4 EN-DE, 41.0 EN-FR) with **8 GPUs in 3.5 days**, far less than prior models.\n",
       "\n",
       "6. **Interpretability**:  \n",
       "   - Attention visualizations show heads capturing syntactic/semantic features (e.g., verb-object relationships).\n",
       "\n",
       "**Impact**: Shows attention alone can match or surpass RNN/CNN-based models while being more scalable, laying the groundwork for modern LLMs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_markdown(c([pdf_url, \"Summarize the key innovations in this paper.\"], extra_body=extra_body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with PDF\n",
    "\n",
    "In contrast to the `Call` class, the `Chat` tracks history and token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper introduces the Transformer.  \\nIt replaces recurrence and convolution with a new architecture that relies entirely on attention mechanisms.  \\nThe encoder and decoder stacks consist of layers of multi-head self-attention and point-wise, fully-connected feed-forward networks.  \\nUnlike recurrent models, this allows computation for all positions in parallel, halving training time on modern GPUs.  \\nExperimental results on WMT 2014 English-German and English-French translation achieve new state-of-the-art BLEU scores (28.4 and 41.0 respectively) while costing much less to train than prior ensemble systems.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([pdf_url, \"What is this paper about?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask follow-up questions about the PDF. `Chat` will update history and token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Key advantages of the Transformer (attention-only) over RNN-based sequence transduction models:\\n\\n1. **Massive Parallelization** – no sequential recurrence. All positions can be processed simultaneously, slashing wall-clock training time.\\n2. **Lower Training Cost** – 12 h on 8 P100 GPUs for the base model, <¼ the FLOPs of previous best systems, while achieving higher BLEU scores.\\n3. **Somewhat Lower Per-layer Complexity** for typical NLP sequence lengths – \\u202f\\n   O(n²·d) vs O(n·d²) when n ‹‹ d.\\n4. **Shorter Maximum Path Length** – constant O(1) between any two positions versus O(n) for RNNs, improving gradient flow for long-range dependencies.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"What are the key advantages of this approach over RNNs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'file',\n",
       "    'file': {'filename': 'document.pdf',\n",
       "     'file_data': 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'}},\n",
       "   {'type': 'text', 'text': 'What is this paper about?'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': 'The paper introduces the Transformer.  \\nIt replaces recurrence and convolution with a new architecture that relies entirely on attention mechanisms.  \\nThe encoder and decoder stacks consist of layers of multi-head self-attention and point-wise, fully-connected feed-forward networks.  \\nUnlike recurrent models, this allows computation for all positions in parallel, halving training time on modern GPUs.  \\nExperimental results on WMT 2014 English-German and English-French translation achieve new state-of-the-art BLEU scores (28.4 and 41.0 respectively) while costing much less to train than prior ensemble systems.'},\n",
       " {'role': 'user',\n",
       "  'content': 'What are the key advantages of this approach over RNNs?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Key advantages of the Transformer (attention-only) over RNN-based sequence transduction models:\\n\\n1. **Massive Parallelization** – no sequential recurrence. All positions can be processed simultaneously, slashing wall-clock training time.\\n2. **Lower Training Cost** – 12 h on 8 P100 GPUs for the base model, <¼ the FLOPs of previous best systems, while achieving higher BLEU scores.\\n3. **Somewhat Lower Per-layer Complexity** for typical NLP sequence lengths – \\u202f\\n   O(n²·d) vs O(n·d²) when n ‹‹ d.\\n4. **Shorter Maximum Path Length** – constant O(1) between any two positions versus O(n) for RNNs, improving gradient flow for long-range dependencies.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 18709, 'completion_tokens': 282, 'total_tokens': 18991}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
