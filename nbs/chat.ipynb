{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5439cc4b",
   "metadata": {},
   "source": [
    "# Chat\n",
    "\n",
    "`Chat` is an object for conversational LLM interactions that tracks history and token usage across single or multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1585981a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from irouter import Chat\n",
    "from irouter.base import nb_markdown\n",
    "\n",
    "# Load OPENROUTER_API_KEY from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39974d87",
   "metadata": {},
   "source": [
    "In this notebook we will use free tiers for Moonshot AI's Kimi K2 and Google's Gemma 3N. \n",
    "\n",
    "An overview of all available models can be found by calling `get_all_models`:\n",
    "```python\n",
    "from irouter.base import get_all_models\n",
    "model_slugs = get_all_models()\n",
    "model_slugs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d630975",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"moonshotai/kimi-k2:free\", \"google/gemma-3n-e2b-it:free\"]\n",
    "\n",
    "# Test conversation messages\n",
    "first_message = \"Who played the guitar solo on Steely Dan's Kid Charlemagne?\"\n",
    "second_message = \"What other songs did this guitarist play on with Steely Dan?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0e04c5",
   "metadata": {},
   "source": [
    "# Single Model\n",
    "\n",
    "The simplest way to use `Chat` is with a single LLM by providing a model slug. Unlike `Call`, `Chat` maintains conversation history and tracks token usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8056ef65",
   "metadata": {},
   "source": [
    "In this example we initialize a `Chat` object with the free tier of Moonshot AI's Kimi-K2 LLM.\n",
    "\n",
    "To set the API key you can either set an environment variable for `OPENROUTER_API_KEY` to your project or pass `api_key` when initializing `Chat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b36549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt: You are a helpful assistant.\n",
      "Initial history length: 1\n"
     ]
    }
   ],
   "source": [
    "chat = Chat(model_names[0])\n",
    "print(f\"System prompt: {chat.system}\")\n",
    "print(f\"Initial history length: {len(chat.history(model_names[0]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation_start",
   "metadata": {},
   "source": [
    "Let's start a conversation and see how history is tracked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5470f9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Call._get_resp() missing 1 required positional argument: 'raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response1 = \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_message\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m nb_markdown(response1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/carlo/repositories/irouter/src/irouter/chat.py:43\u001b[39m, in \u001b[36mChat.__call__\u001b[39m\u001b[34m(self, message, extra_headers)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mself\u001b[39m._history[model].append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: message})\n\u001b[32m     42\u001b[39m resps = [\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_resp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_history\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m     45\u001b[39m ]\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model, resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, resps):\n\u001b[32m     48\u001b[39m     msg = resp.choices[\u001b[32m0\u001b[39m].message\n",
      "\u001b[31mTypeError\u001b[39m: Call._get_resp() missing 1 required positional argument: 'raw'"
     ]
    }
   ],
   "source": [
    "response1 = chat(first_message)\n",
    "nb_markdown(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history_check",
   "metadata": {},
   "source": [
    "Now let's check the conversation history and token usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "history_display",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"History length after first message: {len(chat.history(model_names[0]))}\")\n",
    "print(f\"\\nConversation history:\")\n",
    "for i, msg in enumerate(chat.history(model_names[0])):\n",
    "    print(f\"{i + 1}. [{msg['role']}]: {msg['content'][:50]}...\")\n",
    "\n",
    "print(f\"\\nToken usage: {chat.usage[model_names[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "followup",
   "metadata": {},
   "source": [
    "Let's continue the conversation to see how context is maintained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "followup_message",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = chat(second_message)\n",
    "nb_markdown(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "updated_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"History length after second message: {len(chat.history(model_names[0]))}\")\n",
    "print(f\"\\nUpdated conversation history:\")\n",
    "for i, msg in enumerate(chat.history(model_names[0])):\n",
    "    role_color = (\n",
    "        \"\\033[94m\"\n",
    "        if msg[\"role\"] == \"user\"\n",
    "        else \"\\033[92m\"\n",
    "        if msg[\"role\"] == \"assistant\"\n",
    "        else \"\\033[93m\"\n",
    "    )\n",
    "    reset_color = \"\\033[0m\"\n",
    "    print(\n",
    "        f\"{i + 1}. {role_color}[{msg['role']}]{reset_color}: {msg['content'][:80]}...\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nCumulative token usage: {chat.usage[model_names[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778efa79",
   "metadata": {},
   "source": [
    "## Multiple Models\n",
    "\n",
    "Using multiple models with `Chat` allows you to compare responses while maintaining separate conversation histories and tracking usage per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_chat = Chat(model_names, system=\"You are a music expert assistant.\")\n",
    "print(f\"Models: {multi_chat.model}\")\n",
    "print(f\"System prompt: {multi_chat.system}\")\n",
    "print(\n",
    "    f\"Initial histories: {[len(multi_chat.history(model)) for model in multi_chat.model]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f458cc7d",
   "metadata": {},
   "source": [
    "When multiple models are used, `Chat` returns a list of responses and maintains separate histories for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_responses1 = multi_chat(first_message)\n",
    "print(f\"Number of responses: {len(multi_responses1)}\")\n",
    "print(f\"\\nResponses:\")\n",
    "for i, (model, response) in enumerate(zip(multi_chat.model, multi_responses1)):\n",
    "    print(f\"\\n{i + 1}. {model}:\")\n",
    "    print(f\"   {response[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi_history",
   "metadata": {},
   "source": [
    "Let's examine the separate histories and usage for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi_history_display",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in multi_chat.model:\n",
    "    print(f\"\\n=== {model} ===\")\n",
    "    print(f\"History length: {len(multi_chat.history(model))}\")\n",
    "    print(f\"Token usage: {multi_chat.usage[model]}\")\n",
    "    print(f\"Last assistant message: {multi_chat.history(model)[-1]['content'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi_followup",
   "metadata": {},
   "source": [
    "Let's continue the conversation with both models to see how context is maintained separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi_followup_message",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_responses2 = multi_chat(second_message)\n",
    "print(\"Follow-up responses:\")\n",
    "for i, (model, response) in enumerate(zip(multi_chat.model, multi_responses2)):\n",
    "    print(f\"\\n{i + 1}. {model}:\")\n",
    "    nb_markdown(response)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage_tracking",
   "metadata": {},
   "source": [
    "## Usage Tracking Summary\n",
    "\n",
    "One of the key advantages of `Chat` over `Call` is comprehensive usage tracking per model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usage_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== USAGE SUMMARY ===\")\n",
    "total_tokens = 0\n",
    "for model in multi_chat.model:\n",
    "    usage = multi_chat.usage[model]\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"  Conversation turns: {(len(multi_chat.history[model]) - 1) // 2}\")\n",
    "    print(f\"  Prompt tokens: {usage['prompt_tokens']}\")\n",
    "    print(f\"  Completion tokens: {usage['completion_tokens']}\")\n",
    "    print(f\"  Total tokens: {usage['total_tokens']}\")\n",
    "    total_tokens += usage[\"total_tokens\"]\n",
    "\n",
    "print(f\"\\nGrand total tokens across all models: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessing_responses",
   "metadata": {},
   "source": [
    "## Accessing Individual Model Responses\n",
    "\n",
    "You can easily access responses from specific models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual_access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response from specific model by index\n",
    "kimi_response = multi_responses2[0]\n",
    "gemma_response = multi_responses2[1]\n",
    "\n",
    "print(\"Kimi K2 response:\")\n",
    "nb_markdown(kimi_response)\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "print(\"Gemma 3N response:\")\n",
    "nb_markdown(gemma_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "The separate history tracking allows for easy comparison of how different models handle the same conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL COMPARISON ===\")\n",
    "for i, model in enumerate(multi_chat.model):\n",
    "    history = multi_chat.history(model)\n",
    "    usage = multi_chat.usage[model]\n",
    "\n",
    "    print(f\"\\n{i + 1}. {model}:\")\n",
    "    print(f\"   Messages in history: {len(history)}\")\n",
    "    print(\n",
    "        f\"   Average tokens per response: {usage['completion_tokens'] / ((len(history) - 1) // 2):.1f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   Efficiency (completion/prompt ratio): {usage['completion_tokens'] / usage['prompt_tokens']:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c9ab23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "`Chat` provides powerful conversational AI capabilities with:\n",
    "\n",
    "- **History tracking**: Maintains separate conversation histories for each model\n",
    "- **Usage tracking**: Comprehensive token usage statistics per model  \n",
    "- **Multi-model support**: Compare responses from multiple models simultaneously\n",
    "- **Context awareness**: Follow-up questions use full conversation context\n",
    "\n",
    "This makes `Chat` ideal for applications requiring conversational context, usage monitoring, or model comparison workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
